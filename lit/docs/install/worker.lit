\title{\aux{Running a }\code{worker} node}{concourse-worker}{worker-node}

\use-plugin{concourse-docs}

The \code{worker} node registers with the \reference{web-node} and is then used
for executing builds and performing resource \code{check}s. It doesn't really
decide much on its own.

\table-of-contents

\section{
  \title{Prerequisites}{worker-prerequisites}

  \list{
    Linux: We test and support the following distributions. Minimum kernel
    version tested is 4.4. User namespaces must be enabled. To enforce memory
    limits on tasks, memory + swap accounting must be enabled.

    \list{
      Ubuntu 16.04 (kernel 4.4)
    }{
      Ubuntu 18.04 (kernel 5.3)
    }{
      Ubuntu 20.04 (kernel 5.4)
    }{
      Debian 10 (kernel 4.19)
    }
  }{
    Windows/Darwin: no special requirements (that we know of).

    \aside{
      \bold{NOTE:} Windows containers are currently not supported and Darwin
      does not have native containers. Steps will run inside a temporary
      directory on the Windows/Darwin worker. Any dependencies needed for your
      tasks (e.g. git, .NET, golang, ssh) should be pre-installed on the
      worker. Windows/Darwin workers do not come with any resource types.
    }
  }
}

\section{
  \title{Running \code{concourse worker}}{worker-running}

  The \code{concourse} CLI can run as a \code{worker} node via the
  \code{worker} subcommand.

  First, you'll need to configure a directory for the worker to store data:

  \codeblock{bash}{{{
  CONCOURSE_WORK_DIR=/opt/concourse/worker
  }}}

  This is where all the builds run, and where all resources are fetched in to,
  so make sure it's backed by enough storage.

  Next, point the worker at your \reference{web-node} like so:

  \codeblock{bash}{{{
  CONCOURSE_TSA_HOST=10.0.2.15:2222
  CONCOURSE_TSA_PUBLIC_KEY=path/to/tsa_host_key.pub
  CONCOURSE_TSA_WORKER_PRIVATE_KEY=path/to/worker_key
  }}}

  Finally start the worker:

  \codeblock{bash}{{{
  # run with -E to forward env config, or just set it all as root
  sudo -E concourse worker
  }}}

  Note that the worker must be run as \code{root} because it orchestrates
  containers.

  All logs will be emitted to \code{stdout}, with any panics or lower-level
  errors being emitted to \code{stderr}.

  \section{
    \title{Resource utilization}{worker-resource-utilization}

    \bold{CPU usage}: almost entirely subject to pipeline workloads. More
    resources configured will result in more checking, and in-flight builds
    will use as much CPU as they want.

    \bold{Memory usage}: also subject to pipeline workloads. Expect usage to
    increase with the number of containers on the worker and spike as builds
    run.

    \bold{Bandwidth usage}: again, almost entirely subject to pipeline
    workloads. Expect spikes from periodic checking, though the intervals
    should spread out over enough time. Resource fetching and pushing will also
    use arbitrary bandwidth.

    \bold{Disk usage}: arbitrary data will be written as builds run, and
    resource caches will be kept and garbage collected on their own life cycle.
    We suggest going for a larger disk size if it's not too much trouble. All
    state on disk must not outlive the worker itself; it is all ephemeral. If
    the worker is re-created (i.e. fresh VM/container and all processes were
    killed), it should be brought back with an empty disk.

    \bold{Highly available}: not applicable. Workers are inherently singletons,
    as they're being used as drivers running entirely different workloads.

    \bold{Horizontally scalable}: yes; workers directly correlate to your
    capacity required by however many pipelines, resources, and in-flight
    builds you want to run. It makes sense to scale them up and down with
    demand.

    \bold{Outbound traffic}:
    \list{
      External traffic to arbitrary locations as a result of periodic resource
      checking and running builds
    }{
      External traffic to the \code{web} node's configured external URL when
      downloading the inputs for a \reference{fly-execute}
    }{
      External traffic to the \code{web} node's TSA port (\code{2222}) for
      registering the worker
    }

    \bold{Inbound traffic}:
    \list{
      From the \reference{web-node} on port \code{7777} (Garden) and
      \code{7788} (BaggageClaim). These ports do not need to be exposed, they
      are forwarded to the web node via the ssh connection on port 2222.
    }
  }
}

\section{
  \title{Operating a \code{worker} node}{worker-operation}

  The \code{worker} nodes are designed to be stateless and as interchangeable
  as possible. \reference{tasks} and \reference{resources} bring their own
  Docker images, so you should never have to install dependencies on the
  worker. Windows and Darwin workers are the exception to this. Any
  dependencies should be pre-installed on Windows and Darwin workers.

  In Concourse, all important data is represented by \reference{resources}, so
  the workers themselves are dispensible. Any data in the work-dir is ephemeral
  and should go away when the worker machine is removed - it should not be
  persisted between worker VM or container re-creates.

  \section{
    \title{Scaling Workers}

    More workers should be added to accomodate more pipelines. To know when
    this is necessary you should probably set up \reference{metrics} and keep
    an eye on container counts. If it's starting to approach 200 or so, you
    should probably add another worker. Load average is another metric to keep
    an eye on.

    To add a worker, just create another machine for the worker and follow the
    \reference{worker-running} instructions again.

    \aside{
      \bold{Note}: it doesn't make sense to run multiple workers on one machine
      since they'll both be contending for the same physical resources.
      Workers should be given their own VMs or physical machines to maximize
      resource usage.
    }
  }

  \section{
    \title{Worker Heartbeating & Stalling}

    Workers will continuously heartbeat to the Concourse cluster in order to
    remain registered and healthy. If a worker hasn't checked in after a while,
    possibly due to a network error, being overloaded, or having crashed, the
    web node will transition its state to \code{stalled} and new workloads will
    not be scheduled on that worker until it recovers.

    If the worker remains in this state and cannot be recovered, it can be
    removed using the \reference{fly-prune-worker} command.
  }

  \section{
    \title{Restarting a Worker}

    Workers can be restarted in-place by sending \code{SIGTERM} to the worker
    process and starting it back up. Containers will remain running and
    Concourse will reattach to builds that were in flight.

    This is a pretty aggressive way to restart a worker, and may result in
    errored builds - there are a few moving parts involved and we're still
    working on making this airtight.

    A safer way to restart a worker is to \italic{land} it by sending
    \code{SIGUSR1} to the \code{worker} process. This will switch the worker to
    the \code{landing} state and Concourse will stop scheduling new work on it.
    When all builds running on the worker have finished, the process will exit.

    You may want to enforce a timeout for draining - that way a stuck build
    won't prevent your workers from being upgraded. This can be enforced by
    common tools like \code{start-stop-daemon}:

    \codeblock{bash}{{{
    start-stop-daemon \
      --pidfile worker.pid \
      --stop \
      --retry USR1/300/TERM/15/KILL
    }}}

    This will send \code{SIGUSR1}, wait up to 5 minutes, and then send
    \code{SIGTERM}. If it's \italic{still} running, it will be killed after an
    additional 15 seconds.

    Once the timeout is enforced, there's still a chance that builds that were
    running will continue when the worker comes back.
  }

  \section{
    \title{Gracefully Removing a Worker}

    When a worker machine is going away, it should be \italic{retired}. This is
    similar to \italic{landing}, except at the end the worker is completely
    unregistered, along with its volumes and containers. This should be done
    when a worker's VM or container are being destroyed.

    To retire a worker, send \code{SIGUSR2} to the \code{worker} process. This
    will switch the worker to \code{retiring} state, and Concourse will stop
    scheduling new work on it. When all builds running on the worker have
    finished, the worker will be removed and the \code{worker} process will
    exit.

    Just like with landing, you may want to enforce a timeout for draining -
    that way a stuck build won't prevent your workers from being upgraded. This
    can be enforced by common tools like \code{start-stop-daemon}:

    \codeblock{bash}{{{
    start-stop-daemon \
      --pidfile worker.pid \
      --stop \
      --retry USR2/300/TERM/15/KILL
    }}}

    This will send \code{SIGUSR2}, wait up to 5 minutes, and then send
    \code{SIGTERM}. If it's \italic{still} running, it will be killed after an
    additional 15 seconds.
  }
}

\section{
  \title{Configuring the \code{worker} node}{worker-configuration}

  If there's something special about your worker and you'd like to target
  builds at it specifically, you can configure tags like so:

  \codeblock{bash}{{
  CONCOURSE_TAG=tag-1,tag-2
  }}

  A tagged worker is taken out of the default placement logic. To run build
  steps on it, specify the \reference{schema.step.tags}. Or, to perform
  resource \code{check}s on it, specify
  \reference{schema.resource.tags}{\code{tags}} on the resource itself.

  \section{
    \title{Configuring Runtimes}

    The worker can be run with multiple container runtimes -
    \link{containerd}{https://github.com/containerd/containerd/},
    \link{Guardian}{https://github.com/cloudfoundry/guardian}, and
    \link{Houdini}{https://github.com/vito/houdini} (an experimental runtime
    for Darwin and Windows). Only \code{containerd} and \code{Guardian} are
    meant for production use. Starting in v7.4.0, \code{containerd} is the
    default runtime for Concourse.

    \aside{
      \bold{Note about architecture}: The web node (ATC) talks to all 3
      runtimes via a single interface called the
      \link{Garden}{https://github.com/cloudfoundry/garden} server. While
      Guardian comes packaged with a Garden server and its flags in Concourse
      are unfortunately prefixed with \code{--garden-*}, Guardian (a runtime)
      and Garden (an interface and server) are two separate tools. An analogy
      for Garden would be the \link{Container Runtime Interface
      (CRI)}{https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/}
      used in Kubernetes. Kubernetes uses containerd via CRI. Concourse uses
      containerd via Garden.
    }

    \section{
      \title{\bold{\code{containerd} runtime}}

      For versions of Concourse before 7.4.0 the \code{containerd} runtime has
      to be enabled manually by setting \code{--runtime}
      (\code{CONCOURSE_RUNTIME}) to \code{containerd} on the \code{concourse
      worker} command.

      The following is a list of the \code{containerd} runtime specific flags
      for Concourse that can be set. They are all optional and have default
      values.

      \codeblock{ini}{{{
        Containerd Configuration:
          --containerd-config=                               Path to a config file to use for the Containerd daemon. [$CONCOURSE_CONTAINERD_CONFIG]
          --containerd-bin=                                  Path to a containerd executable (non-absolute names get resolved from $PATH). [$CONCOURSE_CONTAINERD_BIN]
          --containerd-init-bin=                             Path to an init executable (non-absolute names get resolved from $PATH). (default: /usr/local/concourse/bin/init) [$CONCOURSE_CONTAINERD_INIT_BIN]
          --containerd-cni-plugins-dir=                      Path to CNI network plugins. (default: /usr/local/concourse/bin) [$CONCOURSE_CONTAINERD_CNI_PLUGINS_DIR]
          --containerd-request-timeout=                      How long to wait for requests to Containerd to complete. 0 means no timeout. (default: 5m) [$CONCOURSE_CONTAINERD_REQUEST_TIMEOUT]
          --containerd-max-containers=                       Max container capacity. 0 means no limit. (default: 250) [$CONCOURSE_CONTAINERD_MAX_CONTAINERS]

        Containerd Container Networking:
          --containerd-external-ip=                          IP address to use to reach container's mapped ports. Autodetected if not specified. [$CONCOURSE_CONTAINERD_EXTERNAL_IP]
          --containerd-dns-server=                           DNS server IP address to use instead of automatically determined servers. Can be specified multiple times. [$CONCOURSE_CONTAINERD_DNS_SERVER]
          --containerd-restricted-network=                   Network ranges to which traffic from containers will be restricted. Can be specified multiple times. [$CONCOURSE_CONTAINERD_RESTRICTED_NETWORK]
          --containerd-network-pool=                         Network range to use for dynamically allocated container subnets. (default: 10.80.0.0/16) [$CONCOURSE_CONTAINERD_NETWORK_POOL]
          --containerd-mtu=                                  MTU size for container network interfaces. Defaults to the MTU of the interface used for outbound access by the host. [$CONCOURSE_CONTAINERD_MTU]
          --containerd-allow-host-access                     Allow containers to reach the host's network. This is turned off by default. [$CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS]

        DNS Proxy Configuration:
          --containerd-dns-proxy-enable                      Enable proxy DNS server. Note: this will enable containers to access the host network. [$CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE]
      }}}

      \warn{
        Make sure to read \reference{note-on-allow-host-access} to understand
        the implications of using \code{--containerd-allow-host-access} and
        \code{--containerd-dns-proxy-enable}
      }
    }
    \section{
      \title{\bold{Transitioning from Guardian to containerd}}

      If you are transitioning from \code{Guardian} to \code{containerd} you
      will need to convert any \code{--garden-*} (\code{CONCOURSE_GARDEN_*})
      flags to their \code{containerd} (\code{CONCOURSE_CONTAINERD_*})
      counterparts:

      \table{
        \table-row{Guardian Flags}{Containerd Flags}
       }{
        \table-row{
          \code{--garden-request-timeout}
          \code{CONCOURSE_GARDEN_REQUEST_TIMEOUT}
        }{
          \code{--containerd-request-timeout}
          \code{CONCOURSE_CONTAINERD_REQUEST_TIMEOUT}
        }
      }{
        \table-row{
          \code{--garden-dns-proxy-enable}
          \code{CONCOURSE_GARDEN_DNS_PROXY_ENABLE}
        }{
          \code{--containerd-dns-proxy-enable}
          \code{CONCOURSE_CONTAINERD_DNS_PROXY_ENABLE}
        }
      }{
        \table-row{
          \code{--garden-allow-host-access}
          \code{CONCOURSE_GARDEN_ALLOW_HOST_ACCESS}
        }{
          \code{--containerd-allow-host-access}
          \code{CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS}
        }
      }{
        \table-row{
          \code{--garden-network-pool}
          \code{CONCOURSE_GARDEN_NETWORK_POOL}
        }{
          \code{--containerd-network-pool}
          \code{CONCOURSE_CONTAINERD_NETWORK_POOL}
        }
      }{
        \table-row{
          \code{--garden-max-containers}
          \code{CONCOURSE_GARDEN_MAX_CONTAINERS}
        }{
          \code{--containerd-max-containers}
          \code{CONCOURSE_CONTAINERD_MAX_CONTAINERS}
        }
      }{
        \table-row{
          \code{--garden-deny-network}
          \code{CONCOURSE_GARDEN_DENY_NETWORKS}
        }{
          \code{--containerd-restricted-network}
          \code{CONCOURSE_CONTAINERD_RESTRICTED_NETWORK}
        }
      }{
        \table-row{
          \code{--garden-dns-server}
          \code{CONCOURSE_GARDEN_DNS_SERVER}
        }{
          \code{--containerd-dns-server}
          \code{CONCOURSE_CONTAINERD_DNS_SERVER}
        }
      }{
        \table-row{
          \code{--garden-external-ip}
          \code{CONCOURSE_GARDEN_EXTERNAL_IP}
        }{
          \code{--containerd-external-ip}
          \code{CONCOURSE_CONTAINERD_EXTERNAL_IP}
        }
      }{
        \table-row{
          \code{--garden-mtu}
          \code{CONCOURSE_GARDEN_MTU}
        }{
          \code{--containerd-mtu}
          \code{CONCOURSE_CONTAINERD_MTU}
        }
      }
    }

    \section{
      \title{\bold{\code{Guardian} runtime}}

      For versions earlier than 7.4.0 Guardian was the default runtime for
      Concourse. To enable it manually in future versions the \code{--runtime}
      flag has to be set to \code{guardian}.

      The \code{concourse worker} command automatically configures and runs
      \code{Guardian} using the \code{gdn} binary, but depending on the
      environment you're running Concourse in, you may need to pop open the
      hood and configure a few things.

      The \code{gdn} server can be configured in two ways:

      \ordered-list{
        By creating a \code{config.ini} file and passing it as
        \code{--garden-config} (or \code{CONCOURSE_GARDEN_CONFIG}).

        The \code{.ini} file should look something like this:

        \codeblock{ini}{{{
        [server]
        flag-name=flag-value
        }}}

        To learn which flags can be set, consult \code{gdn server --help}. Each
        flag listed can be set under the \code{[server]} heading.
      }{
        By setting \code{CONCOURSE_GARDEN_*} environment variables.

        This is primarily supported for backwards compatibility, and these
        variables are not present in \code{concourse worker --help}. They are
        translated to flags passed to \code{gdn server} by lower-casing the
        \code{*} portion and replacing underscores with hyphens.
      }
    }

      \section{
        \title{Troubleshooting and fixing DNS resolution}

        \aside{
          \bold{Note}: The Guardian runtime took care of a lot of container
          creation operations for Concourse in the past. It was very
          user-friendly for the project to use as a container runtime. While
          implementing the containerd runtime most reported bugs were actually
          a difference in containerd's default behaviour compared to
          Guardian's. Currently Concourse's containerd runtime \bold{mostly}
          behaves like the Guardian runtime did. Most of the following DNS
          section should apply to both runtimes.
        }

        By default, containers created by the Guardian or containerd (will
        refer to both as \italic{runtime}) runtime will carry over the
        \code{/etc/resolv.conf} from the host into the container. This is often
        fine, but some Linux distributions configure a special \code{127.x.x.x}
        DNS resolver (e.g. \code{systemd-resolved}).

        When the runtime copies the \code{resolv.conf} over, it removes these
        entries as they won't be reachable from the container's network
        namespace. As a result, your containers may not have any valid
        nameservers configured.

        To diagnose this problem you can \reference{fly-intercept} into a failing
        container and check which nameservers are in \code{/etc/resolv.conf}:

        \codeblock{bash}{{{
        $ fly -t ci intercept -j concourse/concourse
        bash-5.0$ grep nameserver /etc/resolv.conf
        bash-5.0$
        }}}

        In this case it is empty, as the host only listed a single
        \code{127.0.0.53} address which was then stripped out. To fix this
        you'll need to explicitly configure DNS instead of relying on the
        default runtime behavior.

        \section{
          \title{Pointing to external DNS servers}

          If you have no need for special DNS resolution within your Concourse
          containers, you can configure your containers to use specific DNS
          server addresses external to the VM.

          Both the Guardian and containerd runtimes can have their DNS servers
          configured with flags or envs vars.

          \titled-codeblock{DNS Servers via flags}{bash}{{
          concourse worker --containerd-dns-server="1.1.1.1" --containerd-dns-server="8.8.8.8"
          concourse worker --garden-dns-server="1.1.1.1" --garden-dns-server="8.8.8.8"
          }}

          \inset{} {- spacer -}

          \titled-codeblock{DNS Servers via env vars}{bash}{{
          # containerd runtime
          CONCOURSE_CONTAINERD_DNS_SERVER="1.1.1.1,8.8.8.8"
          # Guardian runtime
          CONCOURSE_GARDEN_DNS_SERVER="1.1.1.1,8.8.8.8"
          }}

          \inset{} {- spacer -}

          \titled-codeblock{config.ini (Guardian runtime only)}{ini}{{{
          [server]
          ; configure Google DNS
          dns-server=8.8.8.8
          dns-server=8.8.4.4
          }}}

          To verify this solves your problem you can \reference{fly-intercept}
          into a container and check which nameservers are in
          \code{/etc/resolv.conf}:

          \codeblock{bash}{{{
          $ fly -t ci intercept -j my-pipeline/the-job
          bash-5.0$ cat /etc/resolv.conf
          nameserver 1.1.1.1
          nameserver 8.8.8.8
          bash-5.0$ ping google.com
          PING google.com (108.177.111.139): 56 data bytes
          64 bytes from 108.177.111.139: seq=0 ttl=47 time=2.672 ms
          64 bytes from 108.177.111.139: seq=1 ttl=47 time=0.911 ms
          }}}

        }

        \section{
          \title{Using a local DNS server}

          If you would like to use Consul, \code{dnsmasq}, or some other DNS server
          running on the worker VM, you'll have to configure the LAN address
          of the VM as the DNS server \italic{and} allow the containers to reach
          the address, like so:

          \titled-codeblock{Local DNS Servers via flags}{bash}{{
          concourse worker --containerd-dns-server="10.0.1.3" --containerd-allow-host-access="true"
          concourse worker --garden-dns-server="10.0.1.3" --garden-allow-host-access="true"
          }}

          \inset{} {- spacer -}

          \titled-codeblock{Local DNS Servers via env vars}{bash}{{
          # containerd runtime
          CONCOURSE_CONTAINERD_DNS_SERVER="10.0.1.3"
          CONCOURSE_CONTAINERD_ALLOW_HOST_ACCESS="true"
          # Guardian runtime
          CONCOURSE_GARDEN_DNS_SERVER="10.0.1.3"
          CONCOURSE_GARDEN_ALLOW_HOST_ACCESS="true"
          }}

          \inset{} {- spacer -}

          \titled-codeblock{config.ini (Guardian runtime only)}{ini}{{{
          [server]
          ; internal IP of the worker machine
          dns-server=10.0.1.3

          ; allow containers to reach the above IP
          allow-host-access=true
          }}}

          \inset{} {- spacer -}

          \warn{
            Make sure to read \reference{note-on-allow-host-access} to
            understand the implications of using \code{allow-host-access}
          }

          To validate whether the changes have taken effect, you can
          \reference{fly-intercept} into any container and check
          \code{/etc/resolv.conf} once again:

          \codeblock{bash}{{{
          $ fly -t ci intercept -j my-pipeline/the-job
          bash-5.0$ cat /etc/resolv.conf
          nameserver 10.1.2.3
          bash-5.0$ nslookup concourse-ci.org
          Server:         10.1.2.3
          Address:        10.1.2.3#53

          Non-authoritative answer:
          Name:   concourse-ci.org
          Address: 185.199.108.153
          Name:   concourse-ci.org
          Address: 185.199.109.153
          Name:   concourse-ci.org
          Address: 185.199.110.153
          Name:   concourse-ci.org
          Address: 185.199.111.153
          }}}

          If \code{nslookup} times out or fails, you may need to open up firewalls
          or security group configuration so that the worker VM can send UDP/TCP
          packets to itself.
        }
        \section{
          \title{A note on allowing host access and DNS proxy}{note-on-allow-host-access}

          Setting \code{allow-host-access} will, well, allow containers to access
          your host VM's network. If you don't trust your container workloads,
          you may not want to allow this.  With host network access, containers
          will be able to reach out to any other locally running network
          processes running on the worker including the garden and baggageclaim
          servers \bold{which would allow them to issue commands and manipulate
          other containers and volumes on the same worker}.

          Setting \code{dns-proxy-enable} will also enable
          \code{allow-host-access} (since the dns proxy will be run on the host,
          therefore requiring host access be enabled).

        }
      }
  }

}
